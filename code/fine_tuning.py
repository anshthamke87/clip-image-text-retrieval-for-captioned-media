# -*- coding: utf-8 -*-
"""fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18OHj7-GDKlgJE-ccTtld_MoNYtdM5Lo6
"""

# ================================================================
# CLIP IMAGE-TEXT RETRIEVAL - PHASE 4: FINE-TUNING
# Improve performance with margin ranking loss and hard negatives
# ================================================================

print("ğŸš€ CLIP PROJECT - PHASE 4: FINE-TUNING")
print("=" * 60)
print("ğŸ“Š Goal: Improve textâ†’image R@1 from 50.7% to 60%+")
print("ğŸ”¬ Tasks: Margin loss, hard negatives, ablations, evaluation")
print("=" * 60)

# ================================================================
# 1. SETUP & ENVIRONMENT
# ================================================================

print("\nğŸ”§ SECTION 1: SETUP & ENVIRONMENT")
print("=" * 40)

# Mount Google Drive and navigate to project
from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/MyDrive/clip_image_text_retrieval')
print(f"ğŸ“ Working directory: {os.getcwd()}")

# Install required packages
print("ğŸ“¦ Installing required packages...")
#!pip install open-clip-torch datasets torch torchvision -q

# Import essential libraries
import json
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import open_clip
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import time
import pickle
import random
from pathlib import Path
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

set_seed(42)

# Check device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”¥ Device: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

print("âœ… Environment setup complete!")

# ================================================================
# 2. LOAD DATASET AND BASELINE RESULTS
# ================================================================

print("\nğŸ“Š SECTION 2: LOAD DATASET AND BASELINE RESULTS")
print("=" * 40)

def load_dataset_split(split_name):
    """Load a dataset split from JSONL file"""
    file_path = f'data/{split_name}.jsonl'
    data = []

    print(f"ğŸ“– Loading {split_name} split from {file_path}...")

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line.strip()))

    print(f"   âœ… Loaded {len(data)} entries")
    return data

# Load all splits
train_data = load_dataset_split('train')
val_data = load_dataset_split('val')
test_data = load_dataset_split('test')

print(f"\nğŸ“Š Dataset Statistics:")
print(f"   Train: {len(train_data)} entries")
print(f"   Val:   {len(val_data)} entries")
print(f"   Test:  {len(test_data)} entries")

# Load baseline results for comparison
print(f"\nğŸ“Š Loading baseline results...")
with open('results/zero_shot_baseline_results.json', 'r') as f:
    baseline_results = json.load(f)

print(f"âœ… Baseline performance loaded:")
print(f"   Textâ†’Image R@1: {baseline_results['test_results']['text_to_image']['Recall@1']:.3f}")
print(f"   Imageâ†’Text R@1: {baseline_results['test_results']['image_to_text']['Recall@1']:.3f}")

# ================================================================
# 3. LOAD OPENCLIP MODEL FOR FINE-TUNING
# ================================================================

print("\nğŸ¤– SECTION 3: LOAD OPENCLIP MODEL FOR FINE-TUNING")
print("=" * 40)

# Load OpenCLIP model (same as Phase 2)
model_name = 'ViT-B-32'
pretrained = 'openai'

print(f"ğŸ“¥ Loading OpenCLIP model: {model_name} ({pretrained})")

model, _, preprocess = open_clip.create_model_and_transforms(
    model_name,
    pretrained=pretrained,
    device=device
)
tokenizer = open_clip.get_tokenizer(model_name)

print("âœ… Model loaded successfully!")

# Model parameters info
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"ğŸ“Š Model info:")
print(f"   Total parameters: {total_params:,}")
print(f"   Trainable parameters: {trainable_params:,}")
print(f"   Model device: {next(model.parameters()).device}")

# ================================================================
# 4. CREATE DATASET AND DATALOADER
# ================================================================

print("\nğŸ“Š SECTION 4: CREATE DATASET AND DATALOADER")
print("=" * 40)

class FlickrDataset(Dataset):
    """Dataset for CLIP fine-tuning with image-caption pairs"""

    def __init__(self, data, preprocess, tokenizer, max_length=77):
        self.data = data
        self.preprocess = preprocess
        self.tokenizer = tokenizer
        self.max_length = max_length

        # Group data by image for efficient sampling
        self.image_to_captions = defaultdict(list)
        for item in data:
            self.image_to_captions[item['image_id']].append(item)

        # Create list of (image_id, caption_data) pairs
        self.pairs = []
        for item in data:
            self.pairs.append(item)

        print(f"   âœ… Dataset created with {len(self.pairs)} pairs")
        print(f"   ğŸ“Š Unique images: {len(self.image_to_captions)}")

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        item = self.pairs[idx]

        # Load and preprocess image
        image_path = item['image_path']
        try:
            with Image.open(image_path) as img:
                image = self.preprocess(img)
        except Exception as e:
            print(f"Error loading image {image_path}: {e}")
            # Return dummy image
            image = torch.zeros(3, 224, 224)

        # Tokenize caption
        caption = item['caption']
        text_tokens = self.tokenizer([caption])[0]  # Get first (and only) item

        return {
            'image': image,
            'text_tokens': text_tokens,
            'caption': caption,
            'image_id': item['image_id'],
            'caption_id': item['caption_id']
        }

# Create datasets
print("ğŸ”§ Creating datasets...")

train_dataset = FlickrDataset(train_data, preprocess, tokenizer)
val_dataset = FlickrDataset(val_data, preprocess, tokenizer)

# Create dataloaders
batch_size = 64  # Adjust based on GPU memory
num_workers = 2

train_dataloader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    pin_memory=True if torch.cuda.is_available() else False
)

val_dataloader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=num_workers,
    pin_memory=True if torch.cuda.is_available() else False
)

print(f"âœ… Dataloaders created:")
print(f"   Train batches: {len(train_dataloader)}")
print(f"   Val batches: {len(val_dataloader)}")
print(f"   Batch size: {batch_size}")

# ================================================================
# 5. DEFINE TRAINING COMPONENTS
# ================================================================

print("\nğŸ¯ SECTION 5: DEFINE TRAINING COMPONENTS")
print("=" * 40)

class CLIPFineTuner(nn.Module):
    """Wrapper for CLIP model fine-tuning"""

    def __init__(self, clip_model, temperature=0.07):
        super().__init__()
        self.clip_model = clip_model
        self.temperature = temperature

    def forward(self, images, text_tokens):
        # Encode images and text
        image_features = self.clip_model.encode_image(images)
        text_features = self.clip_model.encode_text(text_tokens)

        # Normalize features
        image_features = F.normalize(image_features, dim=-1)
        text_features = F.normalize(text_features, dim=-1)

        return image_features, text_features

    def compute_similarity(self, image_features, text_features):
        """Compute similarity matrix"""
        return torch.matmul(image_features, text_features.T) / self.temperature

# Wrap model for fine-tuning
fine_tuning_model = CLIPFineTuner(model, temperature=0.07).to(device)

# Define loss function (InfoNCE / Contrastive Loss)
def contrastive_loss(similarity_matrix):
    """Compute InfoNCE contrastive loss"""
    batch_size = similarity_matrix.shape[0]

    # Labels are diagonal (each image matches its caption)
    labels = torch.arange(batch_size, device=similarity_matrix.device)

    # Image-to-text loss
    loss_i2t = F.cross_entropy(similarity_matrix, labels)

    # Text-to-image loss
    loss_t2i = F.cross_entropy(similarity_matrix.T, labels)

    # Combined loss
    total_loss = (loss_i2t + loss_t2i) / 2

    return total_loss, loss_i2t, loss_t2i

# Setup optimizer
learning_rate = 5e-6  # Small LR for fine-tuning
weight_decay = 1e-4

# Only train the projection heads (keep visual/text encoders frozen initially)
optimizer = optim.AdamW(fine_tuning_model.parameters(), lr=learning_rate, weight_decay=weight_decay)

# Learning rate scheduler
num_epochs = 5
num_training_steps = len(train_dataloader) * num_epochs
warmup_steps = int(0.1 * num_training_steps)  # 10% warmup

scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer, T_0=num_training_steps, eta_min=1e-7
)

print(f"âœ… Training components ready:")
print(f"   Learning rate: {learning_rate}")
print(f"   Weight decay: {weight_decay}")
print(f"   Scheduler: CosineAnnealingWarmRestarts")
print(f"   Training steps: {num_training_steps}")
print(f"   Warmup steps: {warmup_steps}")

# ================================================================
# 6. EVALUATION FUNCTIONS
# ================================================================

print("\nğŸ“Š SECTION 6: EVALUATION FUNCTIONS")
print("=" * 40)

@torch.no_grad()
def evaluate_model(model, dataloader, device, max_batches=None):
    """Evaluate model on validation set"""
    model.eval()

    all_image_features = []
    all_text_features = []
    all_image_ids = []

    print("ğŸ” Computing embeddings for evaluation...")

    for i, batch in enumerate(tqdm(dataloader, desc="Evaluation")):
        if max_batches and i >= max_batches:
            break

        images = batch['image'].to(device)
        text_tokens = batch['text_tokens'].to(device)

        # Get features
        image_features, text_features = model(images, text_tokens)

        all_image_features.append(image_features.cpu())
        all_text_features.append(text_features.cpu())
        all_image_ids.extend(batch['image_id'])

    # Concatenate all features
    all_image_features = torch.cat(all_image_features, dim=0)
    all_text_features = torch.cat(all_text_features, dim=0)

    print(f"ğŸ“Š Evaluation embeddings: {all_image_features.shape[0]} image, {all_text_features.shape[0]} text")

    return all_image_features, all_text_features, all_image_ids

def compute_retrieval_metrics(image_features, text_features, image_ids, k_values=[1, 5, 10]):
    """Compute retrieval metrics from features"""

    # Group text features by image_id
    image_to_text_indices = defaultdict(list)
    for i, img_id in enumerate(image_ids):
        image_to_text_indices[img_id].append(i)

    # Get unique images
    unique_image_ids = list(image_to_text_indices.keys())

    # Create mapping from text index to image index
    text_to_image_idx = []
    for i, img_id in enumerate(image_ids):
        image_idx = unique_image_ids.index(img_id)
        text_to_image_idx.append(image_idx)

    # Compute similarity matrix
    similarity_matrix = torch.matmul(text_features, image_features.T)

    # TEXT -> IMAGE RETRIEVAL
    text_ranks = []
    for i, correct_img_idx in enumerate(text_to_image_idx):
        similarities = similarity_matrix[i]
        ranked_indices = torch.argsort(similarities, descending=True)
        rank = (ranked_indices == correct_img_idx).nonzero(as_tuple=True)[0].item() + 1
        text_ranks.append(rank)

    # Compute text->image metrics
    text_to_image_results = {}
    for k in k_values:
        recall_at_k = np.mean([1 if rank <= k else 0 for rank in text_ranks])
        text_to_image_results[f'Recall@{k}'] = recall_at_k

    text_to_image_results['Median_Rank'] = np.median(text_ranks)

    # IMAGE -> TEXT RETRIEVAL
    image_ranks = []
    for i, img_id in enumerate(unique_image_ids):
        correct_text_indices = image_to_text_indices[img_id]

        # Get similarities for this image
        similarities = similarity_matrix[:, i]
        ranked_indices = torch.argsort(similarities, descending=True)

        # Find best rank among correct texts
        ranks_of_correct = []
        for correct_idx in correct_text_indices:
            rank = (ranked_indices == correct_idx).nonzero(as_tuple=True)[0].item() + 1
            ranks_of_correct.append(rank)

        best_rank = min(ranks_of_correct)
        image_ranks.append(best_rank)

    # Compute image->text metrics
    image_to_text_results = {}
    for k in k_values:
        recall_at_k = np.mean([1 if rank <= k else 0 for rank in image_ranks])
        image_to_text_results[f'Recall@{k}'] = recall_at_k

    image_to_text_results['Median_Rank'] = np.median(image_ranks)

    return {
        'text_to_image': text_to_image_results,
        'image_to_text': image_to_text_results
    }

print("âœ… Evaluation functions ready!")

# ================================================================
# 7. TRAINING LOOP
# ================================================================

print("\nğŸ¯ SECTION 7: TRAINING LOOP")
print("=" * 40)

# Training tracking
training_stats = {
    'epoch': [],
    'train_loss': [],
    'val_loss': [],
    'val_text_to_image_r1': [],
    'val_image_to_text_r1': [],
    'learning_rate': []
}

best_val_recall = 0.0
best_model_state = None

print(f"ğŸš€ Starting fine-tuning for {num_epochs} epochs...")
print(f"ğŸ“Š Target: Improve textâ†’image R@1 from {baseline_results['test_results']['text_to_image']['Recall@1']:.3f}")

for epoch in range(num_epochs):
    print(f"\nğŸ“ˆ EPOCH {epoch + 1}/{num_epochs}")
    print("=" * 30)

    # TRAINING
    fine_tuning_model.train()
    train_losses = []

    pbar = tqdm(train_dataloader, desc=f"Training Epoch {epoch+1}")
    for batch_idx, batch in enumerate(pbar):
        optimizer.zero_grad()

        images = batch['image'].to(device)
        text_tokens = batch['text_tokens'].to(device)

        # Forward pass
        image_features, text_features = fine_tuning_model(images, text_tokens)

        # Compute similarity and loss
        similarity_matrix = fine_tuning_model.compute_similarity(image_features, text_features)
        loss, loss_i2t, loss_t2i = contrastive_loss(similarity_matrix)

        # Backward pass
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(fine_tuning_model.parameters(), max_norm=1.0)

        optimizer.step()
        scheduler.step()

        train_losses.append(loss.item())

        # Update progress bar
        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'LR': f'{scheduler.get_last_lr()[0]:.2e}'
        })

        # Early stopping check (optional - for debugging)
        if batch_idx > 50 and epoch == 0:  # Quick check first epoch
            break

    avg_train_loss = np.mean(train_losses)

    # VALIDATION
    print("ğŸ” Validating...")

    # Quick validation (subset for speed during training)
    val_image_features, val_text_features, val_image_ids = evaluate_model(
        fine_tuning_model, val_dataloader, device, max_batches=20
    )

    val_metrics = compute_retrieval_metrics(
        val_image_features, val_text_features, val_image_ids
    )

    val_text_r1 = val_metrics['text_to_image']['Recall@1']
    val_image_r1 = val_metrics['image_to_text']['Recall@1']

    # Track best model
    if val_text_r1 > best_val_recall:
        best_val_recall = val_text_r1
        best_model_state = fine_tuning_model.state_dict().copy()
        print(f"ğŸ¯ New best validation R@1: {best_val_recall:.3f}")

    # Log training stats
    training_stats['epoch'].append(epoch + 1)
    training_stats['train_loss'].append(avg_train_loss)
    training_stats['val_text_to_image_r1'].append(val_text_r1)
    training_stats['val_image_to_text_r1'].append(val_image_r1)
    training_stats['learning_rate'].append(scheduler.get_last_lr()[0])

    print(f"ğŸ“Š Epoch {epoch + 1} Results:")
    print(f"   Train Loss: {avg_train_loss:.4f}")
    print(f"   Val Textâ†’Image R@1: {val_text_r1:.3f}")
    print(f"   Val Imageâ†’Text R@1: {val_image_r1:.3f}")
    print(f"   Learning Rate: {scheduler.get_last_lr()[0]:.2e}")

print(f"\nğŸ‰ Training complete!")
print(f"ğŸ¯ Best validation R@1: {best_val_recall:.3f}")

# Load best model
if best_model_state:
    fine_tuning_model.load_state_dict(best_model_state)
    print("âœ… Loaded best model weights")

# ================================================================
# 8. FINAL EVALUATION
# ================================================================

print("\nğŸ§ª SECTION 8: FINAL EVALUATION")
print("=" * 40)

# Evaluate on full validation set
print("ğŸ” Full validation evaluation...")
val_image_features, val_text_features, val_image_ids = evaluate_model(
    fine_tuning_model, val_dataloader, device
)

val_results = compute_retrieval_metrics(val_image_features, val_text_features, val_image_ids)

# Evaluate on test set
print("ğŸ” Test set evaluation...")
test_dataset_for_eval = FlickrDataset(test_data, preprocess, tokenizer)
test_dataloader_for_eval = DataLoader(
    test_dataset_for_eval, batch_size=batch_size, shuffle=False, num_workers=num_workers
)

test_image_features, test_text_features, test_image_ids = evaluate_model(
    fine_tuning_model, test_dataloader_for_eval, device
)

test_results = compute_retrieval_metrics(test_image_features, test_text_features, test_image_ids)

# Display results
print(f"\nğŸ“Š FINAL RESULTS:")
print(f"=" * 40)

print(f"ğŸ” VALIDATION SET:")
print(f"   Textâ†’Image R@1: {val_results['text_to_image']['Recall@1']:.3f}")
print(f"   Textâ†’Image R@5: {val_results['text_to_image']['Recall@5']:.3f}")
print(f"   Imageâ†’Text R@1: {val_results['image_to_text']['Recall@1']:.3f}")
print(f"   Imageâ†’Text R@5: {val_results['image_to_text']['Recall@5']:.3f}")

print(f"\nğŸ” TEST SET:")
print(f"   Textâ†’Image R@1: {test_results['text_to_image']['Recall@1']:.3f}")
print(f"   Textâ†’Image R@5: {test_results['text_to_image']['Recall@5']:.3f}")
print(f"   Imageâ†’Text R@1: {test_results['image_to_text']['Recall@1']:.3f}")
print(f"   Imageâ†’Text R@5: {test_results['image_to_text']['Recall@5']:.3f}")

# Compare with baseline
baseline_text_r1 = baseline_results['test_results']['text_to_image']['Recall@1']
baseline_image_r1 = baseline_results['test_results']['image_to_text']['Recall@1']

improvement_text = test_results['text_to_image']['Recall@1'] - baseline_text_r1
improvement_image = test_results['image_to_text']['Recall@1'] - baseline_image_r1

print(f"\nğŸ“ˆ IMPROVEMENT vs BASELINE:")
print(f"   Textâ†’Image R@1: {baseline_text_r1:.3f} â†’ {test_results['text_to_image']['Recall@1']:.3f} ({improvement_text:+.3f})")
print(f"   Imageâ†’Text R@1: {baseline_image_r1:.3f} â†’ {test_results['image_to_text']['Recall@1']:.3f} ({improvement_image:+.3f})")

# Since our model just broke down after finetuning, below is our much mellowed down finetune

# ================================================================
# CONSERVATIVE FINE-TUNING APPROACH - FIX FOR PHASE 4
# Much more careful approach to avoid catastrophic forgetting
# ================================================================

print("ğŸ”§ CONSERVATIVE FINE-TUNING APPROACH")
print("=" * 50)

# 1. RELOAD FRESH MODEL
print("ğŸ”„ Reloading fresh pre-trained model...")

# Reload clean model (avoid corrupted weights)
model, _, preprocess = open_clip.create_model_and_transforms(
    'ViT-B-32',
    pretrained='openai',
    device=device
)
tokenizer = open_clip.get_tokenizer('ViT-B-32')

print("âœ… Fresh model loaded")

# 2. FREEZE MOST LAYERS (CRITICAL CHANGE)
print("â„ï¸ Freezing encoder layers...")

# Freeze visual encoder (keep pre-trained image understanding)
for param in model.visual.parameters():
    param.requires_grad = False

# Freeze text transformer (keep pre-trained text understanding)
for param in model.transformer.parameters():
    param.requires_grad = False

# Only train final projection layers and layer norms
trainable_params = []
for name, param in model.named_parameters():
    if any(layer_name in name for layer_name in ['ln_final', 'text_projection', 'visual.proj']):
        param.requires_grad = True
        trainable_params.append(name)
    else:
        param.requires_grad = False

print(f"âœ… Trainable layers: {trainable_params}")

# Count trainable parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"ğŸ“Š Total params: {total_params:,}")
print(f"ğŸ“Š Trainable params: {trainable_params_count:,} ({trainable_params_count/total_params*100:.1f}%)")

# 3. MUCH MORE CONSERVATIVE TRAINING SETUP
print("âš™ï¸ Setting up conservative training...")

# Wrap model
fine_tuning_model = CLIPFineTuner(model, temperature=0.07).to(device)

# MUCH smaller learning rate (50x smaller than before)
learning_rate = 1e-7
weight_decay = 1e-5  # Also smaller

# More conservative optimizer
optimizer = optim.AdamW(
    [p for p in fine_tuning_model.parameters() if p.requires_grad],
    lr=learning_rate,
    weight_decay=weight_decay,
    betas=(0.9, 0.999),  # More stable
    eps=1e-8
)

# Simple scheduler (no aggressive changes)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.95)

print(f"âœ… Conservative setup:")
print(f"   Learning rate: {learning_rate}")
print(f"   Weight decay: {weight_decay}")
print(f"   Only training: projection layers + layer norms")

# 4. SHORTER, SAFER TRAINING
print("ğŸ¯ Conservative training loop...")

# Much shorter training (2 epochs max)
num_epochs = 2
training_stats_v2 = {
    'epoch': [],
    'train_loss': [],
    'val_text_to_image_r1': [],
    'learning_rate': []
}

best_val_recall_v2 = 0.0
best_model_state_v2 = None

for epoch in range(num_epochs):
    print(f"\nğŸ“ˆ CONSERVATIVE EPOCH {epoch + 1}/{num_epochs}")
    print("=" * 30)

    # Training
    fine_tuning_model.train()
    train_losses = []

    pbar = tqdm(train_dataloader, desc=f"Conservative Training {epoch+1}")
    for batch_idx, batch in enumerate(pbar):
        optimizer.zero_grad()

        images = batch['image'].to(device)
        text_tokens = batch['text_tokens'].to(device)

        # Forward pass
        image_features, text_features = fine_tuning_model(images, text_tokens)

        # Compute similarity and loss
        similarity_matrix = fine_tuning_model.compute_similarity(image_features, text_features)
        loss, loss_i2t, loss_t2i = contrastive_loss(similarity_matrix)

        # Backward pass
        loss.backward()

        # Aggressive gradient clipping (prevent big updates)
        torch.nn.utils.clip_grad_norm_(fine_tuning_model.parameters(), max_norm=0.1)

        optimizer.step()
        scheduler.step()

        train_losses.append(loss.item())

        # Update progress bar
        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'LR': f'{scheduler.get_last_lr()[0]:.2e}'
        })

        # Stop early if loss gets too low (prevent overfitting)
        if loss.item() < 0.05:
            print("ğŸ›‘ Stopping early - loss getting very low")
            break

    avg_train_loss = np.mean(train_losses)

    # Quick validation check
    print("ğŸ” Quick validation check...")

    # Small validation subset
    val_image_features, val_text_features, val_image_ids = evaluate_model(
        fine_tuning_model, val_dataloader, device, max_batches=50
    )

    val_metrics = compute_retrieval_metrics(
        val_image_features, val_text_features, val_image_ids
    )

    val_text_r1 = val_metrics['text_to_image']['Recall@1']

    print(f"ğŸ“Š Epoch {epoch + 1} Results:")
    print(f"   Train Loss: {avg_train_loss:.4f}")
    print(f"   Val Textâ†’Image R@1: {val_text_r1:.3f}")
    print(f"   Learning Rate: {scheduler.get_last_lr()[0]:.2e}")

    # Track best model
    if val_text_r1 > best_val_recall_v2:
        best_val_recall_v2 = val_text_r1
        best_model_state_v2 = fine_tuning_model.state_dict().copy()
        print(f"ğŸ¯ New best validation R@1: {best_val_recall_v2:.3f}")

    # Early stopping if we're doing well
    if val_text_r1 > 0.4:  # If we reach decent performance, stop
        print("âœ… Reached good performance, stopping early")
        break

    # Stop if performance is terrible (model broken)
    if epoch > 0 and val_text_r1 < 0.1:
        print("ğŸ›‘ Performance still terrible, stopping")
        break

    training_stats_v2['epoch'].append(epoch + 1)
    training_stats_v2['train_loss'].append(avg_train_loss)
    training_stats_v2['val_text_to_image_r1'].append(val_text_r1)
    training_stats_v2['learning_rate'].append(scheduler.get_last_lr()[0])

print(f"\nğŸ‰ Conservative training complete!")

# Load best model if we found one
if best_model_state_v2:
    fine_tuning_model.load_state_dict(best_model_state_v2)
    print("âœ… Loaded best conservative model")

# 5. FINAL CONSERVATIVE EVALUATION
print("\nğŸ§ª Final conservative evaluation...")

# Full validation evaluation
val_image_features, val_text_features, val_image_ids = evaluate_model(
    fine_tuning_model, val_dataloader, device
)
val_results_v2 = compute_retrieval_metrics(val_image_features, val_text_features, val_image_ids)

# Test evaluation
test_image_features, test_text_features, test_image_ids = evaluate_model(
    fine_tuning_model, test_dataloader_for_eval, device
)
test_results_v2 = compute_retrieval_metrics(test_image_features, test_text_features, test_image_ids)

print(f"\nğŸ“Š CONSERVATIVE FINE-TUNING RESULTS:")
print(f"=" * 40)

print(f"ğŸ” TEST SET:")
print(f"   Textâ†’Image R@1: {test_results_v2['text_to_image']['Recall@1']:.3f}")
print(f"   Textâ†’Image R@5: {test_results_v2['text_to_image']['Recall@5']:.3f}")
print(f"   Imageâ†’Text R@1: {test_results_v2['image_to_text']['Recall@1']:.3f}")
print(f"   Imageâ†’Text R@5: {test_results_v2['image_to_text']['Recall@5']:.3f}")

# Compare with baseline
baseline_text_r1 = 0.507
baseline_image_r1 = 0.700

improvement_text_v2 = test_results_v2['text_to_image']['Recall@1'] - baseline_text_r1
improvement_image_v2 = test_results_v2['image_to_text']['Recall@1'] - baseline_image_r1

print(f"\nğŸ“ˆ CONSERVATIVE IMPROVEMENT vs BASELINE:")
print(f"   Textâ†’Image R@1: {baseline_text_r1:.3f} â†’ {test_results_v2['text_to_image']['Recall@1']:.3f} ({improvement_text_v2:+.3f})")
print(f"   Imageâ†’Text R@1: {baseline_image_r1:.3f} â†’ {test_results_v2['image_to_text']['Recall@1']:.3f} ({improvement_image_v2:+.3f})")

if test_results_v2['text_to_image']['Recall@1'] > 0.4:
    print("ğŸ‰ SUCCESS: Conservative approach preserved model performance!")
    if improvement_text_v2 > 0:
        print("ğŸ¯ BONUS: Even achieved some improvement!")
elif test_results_v2['text_to_image']['Recall@1'] > 0.2:
    print("âœ… PARTIAL SUCCESS: Model still functional, some performance retained")
else:
    print("âŒ Still broken - may need to stick with baseline model")

print("âœ… Conservative fine-tuning attempt complete!")

# ================================================================
# SECTION 9: SAVE BASELINE MODEL (FINE-TUNING DIDN'T IMPROVE)
# ================================================================

print("\nğŸ’¾ SECTION 9: SAVE BASELINE MODEL")
print("=" * 40)

print("ğŸ“Š Fine-tuning analysis:")
print("   Original baseline: 50.7% textâ†’image R@1")
print("   Fine-tuning attempts: 0.0% (catastrophic forgetting)")
print("   Decision: Keep excellent baseline model")

# Reload the original baseline model (clean)
print("ğŸ”„ Loading original baseline model...")

baseline_model, _, baseline_preprocess = open_clip.create_model_and_transforms(
    'ViT-B-32',
    pretrained='openai',
    device=device
)
baseline_tokenizer = open_clip.get_tokenizer('ViT-B-32')

print("âœ… Original baseline model loaded")

# Save the baseline model as our "best" model
os.makedirs('artifacts/models', exist_ok=True)

torch.save({
    'model_state_dict': baseline_model.state_dict(),
    'model_config': {
        'model_name': 'ViT-B-32',
        'pretrained': 'openai',
        'performance': {
            'text_to_image_recall_1': 0.507,
            'image_to_text_recall_1': 0.700,
            'text_to_image_recall_5': 0.769,
            'image_to_text_recall_5': 0.891
        },
        'notes': 'Original OpenCLIP baseline - fine-tuning attempts unsuccessful'
    },
    'preprocessing': {
        'image_preprocess': 'OpenCLIP standard',
        'text_tokenizer': 'ViT-B-32 tokenizer'
    }
}, 'artifacts/models/baseline_clip_model.pt')

print("âœ… Baseline model saved to artifacts/models/baseline_clip_model.pt")

# Create final results using baseline performance
final_results = {
    'model': 'OpenCLIP ViT-B-32 (Baseline - Fine-tuning Unsuccessful)',
    'dataset': 'Flickr8k',
    'approach': 'Zero-shot baseline (fine-tuning attempts failed)',
    'final_performance': {
        'test_results': baseline_results['test_results'],  # Use original baseline results
        'validation_results': baseline_results['validation_results']
    },
    'fine_tuning_attempts': {
        'attempt_1': {
            'approach': 'Full model fine-tuning',
            'learning_rate': 5e-6,
            'result': 'Catastrophic forgetting (0.0% R@1)',
            'issue': 'Learning rate too high, full model training'
        },
        'attempt_2': {
            'approach': 'Conservative fine-tuning (frozen encoders)',
            'learning_rate': 1e-7,
            'result': 'Still unsuccessful (0.0% R@1)',
            'issue': 'CLIP appears very sensitive to fine-tuning on this dataset'
        }
    },
    'conclusions': {
        'baseline_performance': 'Excellent (50.7% textâ†’image R@1)',
        'fine_tuning_difficulty': 'CLIP fine-tuning is challenging and dataset-dependent',
        'final_recommendation': 'Use strong baseline model for production',
        'lessons_learned': [
            'OpenCLIP baseline already performs very well',
            'Fine-tuning vision-language models requires extreme care',
            'Strong baselines are often sufficient for real applications',
            'Catastrophic forgetting is a real risk with aggressive fine-tuning'
        ]
    },
    'evaluation_date': pd.Timestamp.now().isoformat()
}

# Save comprehensive results
with open('results/final_model_results.json', 'w') as f:
    json.dump(final_results, f, indent=2, default=str)

print("âœ… Final results saved to results/final_model_results.json")

# Create a summary plot comparing baseline vs fine-tuning attempts
plt.figure(figsize=(12, 6))

# Performance comparison
metrics = ['Textâ†’Image R@1', 'Textâ†’Image R@5', 'Imageâ†’Text R@1', 'Imageâ†’Text R@5']
baseline_scores = [0.507, 0.769, 0.700, 0.891]
finetuning_scores = [0.000, 0.001, 0.001, 0.001]  # From the failed attempts

x = np.arange(len(metrics))
width = 0.35

plt.subplot(1, 2, 1)
plt.bar(x - width/2, baseline_scores, width, label='Baseline (OpenCLIP)', color='green', alpha=0.7)
plt.bar(x + width/2, finetuning_scores, width, label='Fine-tuning Attempts', color='red', alpha=0.7)
plt.xlabel('Metrics')
plt.ylabel('Recall Score')
plt.title('Baseline vs Fine-tuning Performance')
plt.xticks(x, metrics, rotation=45)
plt.legend()
plt.grid(True, alpha=0.3)

# Lessons learned text
plt.subplot(1, 2, 2)
plt.text(0.1, 0.9, 'Key Insights:', fontsize=14, fontweight='bold', transform=plt.gca().transAxes)
insights = [
    'â€¢ Baseline: 50.7% textâ†’image R@1',
    'â€¢ Fine-tuning: Catastrophic forgetting',
    'â€¢ OpenCLIP pre-training is excellent',
    'â€¢ Strong baselines often sufficient',
    'â€¢ Fine-tuning requires extreme care'
]

for i, insight in enumerate(insights):
    plt.text(0.1, 0.8 - i*0.12, insight, fontsize=11, transform=plt.gca().transAxes)

plt.text(0.1, 0.2, 'Recommendation:\nUse baseline model for production',
         fontsize=12, fontweight='bold', transform=plt.gca().transAxes,
         bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7))

plt.axis('off')

plt.tight_layout()
plt.savefig('reports/baseline_vs_finetuning_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ… Analysis plot saved to reports/baseline_vs_finetuning_analysis.png")

# Update project state to reflect final decision
with open('artifacts/project_state.json', 'r') as f:
    project_state = json.load(f)

project_state.update({
    'phase_completed': 'fine_tuning_attempted',
    'final_model_choice': 'baseline',
    'fine_tuning_results': {
        'attempts': 2,
        'success': False,
        'final_performance': baseline_results['test_results'],
        'recommendation': 'Use baseline model'
    },
    'model_saved': True,
    'next_phase': 'production_api'
})

with open('artifacts/project_state.json', 'w') as f:
    json.dump(project_state, f, indent=2)

print("âœ… Project state updated with final model choice")

print(f"\n" + "="*60)
print("ğŸ¯ FINAL MODEL DECISION")
print("="*60)

print(f"ğŸ“Š Chosen Model: OpenCLIP ViT-B-32 Baseline")
print(f"ğŸ¯ Performance: 50.7% textâ†’image R@1, 70.0% imageâ†’text R@1")
print(f"âœ… Rationale: Excellent baseline performance, fine-tuning risks outweigh benefits")

print(f"\nğŸ“ Final Artifacts:")
print(f"   â€¢ artifacts/models/baseline_clip_model.pt")
print(f"   â€¢ results/final_model_results.json")
print(f"   â€¢ reports/baseline_vs_finetuning_analysis.png")

print(f"\nğŸš€ Ready for Phase 5: Production API!")
print(f"ğŸ’¡ Will deploy the strong baseline model in FastAPI service")

print("âœ… Phase 4 complete with baseline model choice!")
