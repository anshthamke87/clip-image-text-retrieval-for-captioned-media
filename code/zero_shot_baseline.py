# -*- coding: utf-8 -*-
"""zero_shot_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BMk-ukvuVFbAfoUpSbAzjHVGm39qDWYu
"""

# ================================================================
# CLIP IMAGE-TEXT RETRIEVAL - PHASE 2: ZERO-SHOT BASELINE
# Establish baseline performance with OpenCLIP ViT-B/32
# ================================================================

print("ðŸš€ CLIP PROJECT - PHASE 2: ZERO-SHOT BASELINE")
print("=" * 60)
print("ðŸ“Š Goal: Establish baseline retrieval performance with OpenCLIP")
print("ðŸ”¬ Tasks: Encode data, compute metrics, save embeddings")
print("=" * 60)

# ================================================================
# 1. SETUP & ENVIRONMENT
# ================================================================

print("\nðŸ”§ SECTION 1: SETUP & ENVIRONMENT")
print("=" * 40)

# Mount Google Drive and navigate to project
from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/MyDrive/clip_image_text_retrieval')
print(f"ðŸ“ Working directory: {os.getcwd()}")

# Install required packages
print("ðŸ“¦ Installing required packages...")
#!pip install open-clip-torch datasets torch torchvision -q

# Import essential libraries
import json
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from PIL import Image
import open_clip
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
import time
from pathlib import Path
import pickle
from collections import defaultdict

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Check device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ðŸ”¥ Device: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

print("âœ… Environment setup complete!")

# ================================================================
# 2. LOAD DATASET
# ================================================================

print("\nðŸ“Š SECTION 2: LOAD DATASET")
print("=" * 40)

def load_dataset_split(split_name):
    """Load a dataset split from JSONL file"""
    file_path = f'data/{split_name}.jsonl'
    data = []

    print(f"ðŸ“– Loading {split_name} split from {file_path}...")

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line.strip()))

    print(f"   âœ… Loaded {len(data)} entries")
    return data

# Load all splits
train_data = load_dataset_split('train')
val_data = load_dataset_split('val')
test_data = load_dataset_split('test')

# Dataset statistics
print(f"\nðŸ“Š Dataset Statistics:")
print(f"   Train: {len(train_data)} entries")
print(f"   Val:   {len(val_data)} entries")
print(f"   Test:  {len(test_data)} entries")
print(f"   Total: {len(train_data) + len(val_data) + len(test_data)} entries")

# Check unique images per split
def count_unique_images(data):
    return len(set(item['image_id'] for item in data))

train_images = count_unique_images(train_data)
val_images = count_unique_images(val_data)
test_images = count_unique_images(test_data)

print(f"\nðŸ–¼ï¸  Unique Images:")
print(f"   Train: {train_images} images")
print(f"   Val:   {val_images} images")
print(f"   Test:  {test_images} images")
print(f"   Captions per image: ~{len(train_data) / train_images:.1f}")

# Sample data inspection
print(f"\nðŸ“‹ Sample Entry (Train):")
sample = train_data[0]
for key, value in sample.items():
    if key == 'caption':
        print(f"   {key}: {value[:60]}...")
    else:
        print(f"   {key}: {value}")

print("âœ… Dataset loaded successfully!")

# ================================================================
# 3. LOAD OPENCLIP MODEL
# ================================================================

print("\nðŸ¤– SECTION 3: LOAD OPENCLIP MODEL")
print("=" * 40)

# Load OpenCLIP ViT-B/32 model
model_name = 'ViT-B-32'
pretrained = 'openai'

print(f"ðŸ“¥ Loading OpenCLIP model: {model_name} ({pretrained})")

try:
    model, _, preprocess = open_clip.create_model_and_transforms(
        model_name,
        pretrained=pretrained,
        device=device
    )
    tokenizer = open_clip.get_tokenizer(model_name)

    print("âœ… Model loaded successfully!")

    # Model info
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ðŸ“Š Model parameters: {total_params:,}")
    print(f"ðŸ”§ Preprocessing: {type(preprocess).__name__}")

except Exception as e:
    print(f"âŒ Error loading model: {e}")
    raise

# Test model with sample data
print(f"\nðŸ§ª Testing model with sample data...")

try:
    # Test image encoding
    sample_image_path = sample['image_path']
    if os.path.exists(sample_image_path):
        with Image.open(sample_image_path) as img:
            img_tensor = preprocess(img).unsqueeze(0).to(device)

        with torch.no_grad():
            image_features = model.encode_image(img_tensor)
            image_features = F.normalize(image_features, dim=-1)

        print(f"   âœ… Image encoding: {image_features.shape}")
    else:
        print(f"   âš ï¸ Sample image not found: {sample_image_path}")

    # Test text encoding
    sample_text = sample['caption']
    text_tokens = tokenizer([sample_text]).to(device)

    with torch.no_grad():
        text_features = model.encode_text(text_tokens)
        text_features = F.normalize(text_features, dim=-1)

    print(f"   âœ… Text encoding: {text_features.shape}")

    # Compute similarity
    similarity = torch.matmul(image_features, text_features.T).item()
    print(f"   ðŸŽ¯ Image-text similarity: {similarity:.4f}")

except Exception as e:
    print(f"   âŒ Error testing model: {e}")

print("âœ… Model testing complete!")

# ================================================================
# 4. ENCODE IMAGES
# ================================================================

print("\nðŸ–¼ï¸ SECTION 4: ENCODE IMAGES")
print("=" * 40)

def encode_images_batch(data, split_name, batch_size=32):
    """Encode images in batches for efficiency"""

    # Get unique images to avoid encoding duplicates
    unique_images = {}
    for item in data:
        img_id = item['image_id']
        if img_id not in unique_images:
            unique_images[img_id] = item['image_path']

    print(f"ðŸ–¼ï¸ Encoding {len(unique_images)} unique images from {split_name} split...")

    image_embeddings = {}
    failed_images = []

    # Convert to list for batching
    image_items = list(unique_images.items())

    # Process in batches
    for i in tqdm(range(0, len(image_items), batch_size), desc=f"Encoding {split_name} images"):
        batch_items = image_items[i:i + batch_size]
        batch_images = []
        batch_ids = []

        # Load and preprocess batch
        for img_id, img_path in batch_items:
            try:
                if os.path.exists(img_path):
                    with Image.open(img_path) as img:
                        img_tensor = preprocess(img)
                        batch_images.append(img_tensor)
                        batch_ids.append(img_id)
                else:
                    failed_images.append((img_id, "File not found"))
            except Exception as e:
                failed_images.append((img_id, str(e)))

        # Encode batch
        if batch_images:
            batch_tensor = torch.stack(batch_images).to(device)

            with torch.no_grad():
                batch_features = model.encode_image(batch_tensor)
                batch_features = F.normalize(batch_features, dim=-1)

            # Store embeddings
            for img_id, features in zip(batch_ids, batch_features):
                image_embeddings[img_id] = features.cpu().numpy()

    print(f"   âœ… Successfully encoded: {len(image_embeddings)} images")
    if failed_images:
        print(f"   âš ï¸ Failed to encode: {len(failed_images)} images")
        for img_id, error in failed_images[:5]:  # Show first 5 errors
            print(f"      {img_id}: {error}")

    return image_embeddings

# Encode images for each split
val_image_embeddings = encode_images_batch(val_data, 'validation', batch_size=32)
test_image_embeddings = encode_images_batch(test_data, 'test', batch_size=32)

# Save embeddings
os.makedirs('artifacts/embeddings', exist_ok=True)

with open('artifacts/embeddings/val_image_embeddings.pkl', 'wb') as f:
    pickle.dump(val_image_embeddings, f)

with open('artifacts/embeddings/test_image_embeddings.pkl', 'wb') as f:
    pickle.dump(test_image_embeddings, f)

print("ðŸ’¾ Image embeddings saved to artifacts/embeddings/")
print("âœ… Image encoding complete!")

# ================================================================
# 5. ENCODE TEXT
# ================================================================

print("\nðŸ“ SECTION 5: ENCODE TEXT")
print("=" * 40)

def encode_texts_batch(data, split_name, batch_size=64):
    """Encode text captions in batches"""

    print(f"ðŸ“ Encoding {len(data)} captions from {split_name} split...")

    text_embeddings = []
    failed_texts = 0

    # Process in batches
    for i in tqdm(range(0, len(data), batch_size), desc=f"Encoding {split_name} texts"):
        batch_data = data[i:i + batch_size]
        batch_texts = [item['caption'] for item in batch_data]

        try:
            # Tokenize batch
            text_tokens = tokenizer(batch_texts).to(device)

            with torch.no_grad():
                batch_features = model.encode_text(text_tokens)
                batch_features = F.normalize(batch_features, dim=-1)

            # Store embeddings
            for j, features in enumerate(batch_features):
                text_embeddings.append({
                    'caption_id': batch_data[j]['caption_id'],
                    'image_id': batch_data[j]['image_id'],
                    'caption': batch_data[j]['caption'],
                    'embedding': features.cpu().numpy()
                })

        except Exception as e:
            print(f"   âš ï¸ Failed to encode batch {i//batch_size}: {e}")
            failed_texts += len(batch_texts)

    print(f"   âœ… Successfully encoded: {len(text_embeddings)} captions")
    if failed_texts > 0:
        print(f"   âš ï¸ Failed to encode: {failed_texts} captions")

    return text_embeddings

# Encode text for each split
val_text_embeddings = encode_texts_batch(val_data, 'validation')
test_text_embeddings = encode_texts_batch(test_data, 'test')

# Save text embeddings
with open('artifacts/embeddings/val_text_embeddings.pkl', 'wb') as f:
    pickle.dump(val_text_embeddings, f)

with open('artifacts/embeddings/test_text_embeddings.pkl', 'wb') as f:
    pickle.dump(test_text_embeddings, f)

print("ðŸ’¾ Text embeddings saved to artifacts/embeddings/")
print("âœ… Text encoding complete!")

# ================================================================
# 6. EVALUATION METRICS
# ================================================================

print("\nðŸ“Š SECTION 6: EVALUATION METRICS")
print("=" * 40)

def compute_retrieval_metrics(image_embeddings, text_embeddings, k_values=[1, 5, 10]):
    """Compute text->image and image->text retrieval metrics"""

    results = {}

    # Convert embeddings to matrices
    # Group text embeddings by image_id
    image_to_texts = defaultdict(list)
    for text_emb in text_embeddings:
        image_to_texts[text_emb['image_id']].append(text_emb)

    # Get unique images that have both image and text embeddings
    common_image_ids = set(image_embeddings.keys()) & set(image_to_texts.keys())
    print(f"ðŸ“Š Evaluating on {len(common_image_ids)} images")

    if len(common_image_ids) == 0:
        print("âŒ No common images found between image and text embeddings!")
        return results

    # Prepare matrices
    image_ids = list(common_image_ids)
    image_matrix = np.stack([image_embeddings[img_id] for img_id in image_ids])

    # For text->image: collect all text embeddings
    all_text_embeddings = []
    text_to_image_idx = []

    for i, img_id in enumerate(image_ids):
        for text_emb in image_to_texts[img_id]:
            all_text_embeddings.append(text_emb['embedding'])
            text_to_image_idx.append(i)

    text_matrix = np.stack(all_text_embeddings)

    print(f"ðŸ“Š Image matrix shape: {image_matrix.shape}")
    print(f"ðŸ“Š Text matrix shape: {text_matrix.shape}")

    # Compute similarity matrix
    similarity_matrix = np.matmul(text_matrix, image_matrix.T)  # [num_texts, num_images]

    # TEXT -> IMAGE RETRIEVAL
    print(f"\nðŸ” Computing Text -> Image retrieval...")
    text_to_image_results = {}

    text_ranks = []
    for i, correct_img_idx in enumerate(text_to_image_idx):
        # Get similarities for this text query
        similarities = similarity_matrix[i]

        # Rank images by similarity (descending)
        ranked_indices = np.argsort(similarities)[::-1]

        # Find rank of correct image (1-indexed)
        rank = np.where(ranked_indices == correct_img_idx)[0][0] + 1
        text_ranks.append(rank)

    # Compute text->image metrics
    for k in k_values:
        recall_at_k = np.mean([1 if rank <= k else 0 for rank in text_ranks])
        text_to_image_results[f'Recall@{k}'] = recall_at_k

    text_to_image_results['Median_Rank'] = np.median(text_ranks)
    text_to_image_results['Mean_Rank'] = np.mean(text_ranks)

    # IMAGE -> TEXT RETRIEVAL
    print(f"ðŸ” Computing Image -> Text retrieval...")
    image_to_text_results = {}

    image_ranks = []
    for i, img_id in enumerate(image_ids):
        # Get all text indices for this image
        correct_text_indices = [j for j, text_img_idx in enumerate(text_to_image_idx) if text_img_idx == i]

        if not correct_text_indices:
            continue

        # Get similarities for this image query
        similarities = similarity_matrix[:, i]  # All texts' similarity to this image

        # Rank texts by similarity (descending)
        ranked_indices = np.argsort(similarities)[::-1]

        # Find the best rank among correct texts
        ranks_of_correct = []
        for correct_idx in correct_text_indices:
            rank = np.where(ranked_indices == correct_idx)[0][0] + 1
            ranks_of_correct.append(rank)

        # Use the best (lowest) rank
        best_rank = min(ranks_of_correct)
        image_ranks.append(best_rank)

    # Compute image->text metrics
    for k in k_values:
        recall_at_k = np.mean([1 if rank <= k else 0 for rank in image_ranks])
        image_to_text_results[f'Recall@{k}'] = recall_at_k

    image_to_text_results['Median_Rank'] = np.median(image_ranks)
    image_to_text_results['Mean_Rank'] = np.mean(image_ranks)

    results['text_to_image'] = text_to_image_results
    results['image_to_text'] = image_to_text_results
    results['num_queries'] = {
        'text_queries': len(text_ranks),
        'image_queries': len(image_ranks)
    }

    return results

# Evaluate on validation set
print("ðŸ§ª EVALUATING ON VALIDATION SET")
print("=" * 40)

val_results = compute_retrieval_metrics(val_image_embeddings, val_text_embeddings)

if val_results:
    print(f"\nðŸ“Š VALIDATION RESULTS:")
    print(f"=" * 30)

    print(f"ðŸ” Text â†’ Image Retrieval:")
    for metric, value in val_results['text_to_image'].items():
        if 'Recall' in metric:
            print(f"   {metric}: {value:.3f} ({value*100:.1f}%)")
        else:
            print(f"   {metric}: {value:.1f}")

    print(f"\nðŸ” Image â†’ Text Retrieval:")
    for metric, value in val_results['image_to_text'].items():
        if 'Recall' in metric:
            print(f"   {metric}: {value:.3f} ({value*100:.1f}%)")
        else:
            print(f"   {metric}: {value:.1f}")

    print(f"\nðŸ“Š Query Statistics:")
    print(f"   Text queries: {val_results['num_queries']['text_queries']}")
    print(f"   Image queries: {val_results['num_queries']['image_queries']}")

# Evaluate on test set
print(f"\nðŸ§ª EVALUATING ON TEST SET")
print("=" * 40)

test_results = compute_retrieval_metrics(test_image_embeddings, test_text_embeddings)

if test_results:
    print(f"\nðŸ“Š TEST RESULTS:")
    print(f"=" * 30)

    print(f"ðŸ” Text â†’ Image Retrieval:")
    for metric, value in test_results['text_to_image'].items():
        if 'Recall' in metric:
            print(f"   {metric}: {value:.3f} ({value*100:.1f}%)")
        else:
            print(f"   {metric}: {value:.1f}")

    print(f"\nðŸ” Image â†’ Text Retrieval:")
    for metric, value in test_results['image_to_text'].items():
        if 'Recall' in metric:
            print(f"   {metric}: {value:.3f} ({value*100:.1f}%)")
        else:
            print(f"   {metric}: {value:.1f}")

# ================================================================
# 7. SAVE RESULTS
# ================================================================

print(f"\nðŸ’¾ SECTION 7: SAVE RESULTS")
print("=" * 40)

# Save evaluation results
results_data = {
    'model': f'OpenCLIP {model_name} ({pretrained})',
    'dataset': 'Flickr8k',
    'evaluation_date': pd.Timestamp.now().isoformat(),
    'validation_results': val_results,
    'test_results': test_results,
    'model_config': {
        'model_name': model_name,
        'pretrained': pretrained,
        'device': str(device),
        'total_params': total_params
    }
}

os.makedirs('results', exist_ok=True)

with open('results/zero_shot_baseline_results.json', 'w') as f:
    json.dump(results_data, f, indent=2, default=str)

print("âœ… Results saved to results/zero_shot_baseline_results.json")

# Update project state
with open('artifacts/project_state.json', 'r') as f:
    project_state = json.load(f)

project_state.update({
    'phase_completed': 'zero_shot_baseline',
    'baseline_results': {
        'val_text_to_image_recall_1': val_results['text_to_image']['Recall@1'] if val_results else None,
        'test_text_to_image_recall_1': test_results['text_to_image']['Recall@1'] if test_results else None,
    },
    'embeddings_saved': True,
    'next_phase': 'faiss_indexing'
})

with open('artifacts/project_state.json', 'w') as f:
    json.dump(project_state, f, indent=2)

print("âœ… Project state updated")

# ================================================================
# 8. SUMMARY
# ================================================================

print(f"\n" + "="*60)
print("ðŸŽ‰ ZERO-SHOT BASELINE COMPLETE!")
print("="*60)

print(f"ðŸ“Š Summary:")
print(f"   Model: OpenCLIP ViT-B/32 (OpenAI)")
print(f"   Dataset: Flickr8k ({len(common_image_ids) if 'common_image_ids' in locals() else 'N/A'} evaluated images)")
print(f"   Device: {device}")

if test_results:
    print(f"\nðŸŽ¯ Key Test Results:")
    print(f"   Textâ†’Image Recall@1: {test_results['text_to_image']['Recall@1']:.1%}")
    print(f"   Textâ†’Image Recall@5: {test_results['text_to_image']['Recall@5']:.1%}")
    print(f"   Imageâ†’Text Recall@1: {test_results['image_to_text']['Recall@1']:.1%}")
    print(f"   Imageâ†’Text Recall@5: {test_results['image_to_text']['Recall@5']:.1%}")

print(f"\nðŸ“ Artifacts Created:")
print(f"   â€¢ artifacts/embeddings/val_image_embeddings.pkl")
print(f"   â€¢ artifacts/embeddings/test_image_embeddings.pkl")
print(f"   â€¢ artifacts/embeddings/val_text_embeddings.pkl")
print(f"   â€¢ artifacts/embeddings/test_text_embeddings.pkl")
print(f"   â€¢ results/zero_shot_baseline_results.json")

print(f"\nðŸš€ Ready for Phase 3: FAISS Indexing!")
print(f"ðŸ’¡ Next: Build HNSW index for efficient search")

print("âœ… Zero-shot baseline evaluation complete!")