# CLIP Image-Text Retrieval Engine for Captioned Media

A complete end-to-end implementation of bi-directional image-text retrieval using OpenCLIP, FAISS indexing, and production FastAPI deployment.

## üéØ Project Overview

This project implements a complete production pipeline for building a scalable image-text retrieval system that can:
- **Search images using text queries** (text ‚Üí image retrieval)
- **Find relevant captions for images** (image ‚Üí text retrieval)
- **Scale to large datasets** using efficient ANN indexing
- **Deploy in production** with FastAPI and Docker

## üèÜ Final Results

### Production System Performance
**Complete end-to-end system deployed with:**

| Component | Performance | Technology |
|-----------|-------------|------------|
| **Model** | 50.7% text‚Üíimage R@1, 70.0% image‚Üítext R@1 | OpenCLIP ViT-B-32 |
| **Search** | <1ms per query | FAISS HNSW |
| **API** | 8 endpoints, auto-docs | FastAPI |
| **Deployment** | Production-ready | Docker + docker-compose |

### Benchmark Comparison
| Direction | Recall@1 | Recall@5 | Recall@10 | Query Speed |
|-----------|----------|----------|-----------|-------------|
| Text ‚Üí Image | **50.7%** | 76.9% | 85.8% | 0.1ms |
| Image ‚Üí Text | **70.0%** | 89.1% | 94.9% | 0.2ms |

*These results are competitive with published academic papers while maintaining production-grade performance.*

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   FastAPI       ‚îÇ    ‚îÇ   OpenCLIP       ‚îÇ    ‚îÇ   FAISS HNSW    ‚îÇ
‚îÇ   Web Service   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   ViT-B/32       ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Index Search  ‚îÇ
‚îÇ   (8 endpoints) ‚îÇ    ‚îÇ   (50.7% R@1)    ‚îÇ    ‚îÇ   (<1ms query)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚ñº                       ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Docker        ‚îÇ    ‚îÇ   Sub-millisecond‚îÇ    ‚îÇ   8,091 Images  ‚îÇ
‚îÇ   Deployment    ‚îÇ    ‚îÇ   Embeddings     ‚îÇ    ‚îÇ   40,455 Captions‚îÇ
‚îÇ   (Production)  ‚îÇ    ‚îÇ   (512-dim)      ‚îÇ    ‚îÇ   (Flickr8k)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìä Dataset

**Flickr8k (Professionally Processed)**
- **Images**: 8,091 unique images (validated and organized)
- **Captions**: 40,455 total captions (5 per image, normalized)
- **Splits**: 70% train (5,663) / 15% val (1,213) / 15% test (1,215)
- **Quality**: 100% image validation, comprehensive error handling

## üéõÔ∏è Complete Project Pipeline

### ‚úÖ Phase 1: Data Engineering (COMPLETE)
- [x] **Dataset Processing**: Flickr8k download, validation, and cleaning
- [x] **Split Creation**: Proper train/validation/test splits by unique images
- [x] **Quality Assurance**: Image integrity validation and caption normalization
- [x] **Documentation**: Comprehensive dataset documentation with checksums

### ‚úÖ Phase 2: Baseline Establishment (COMPLETE)
- [x] **Model Integration**: OpenCLIP ViT-B-32 implementation
- [x] **Evaluation Pipeline**: Comprehensive Recall@K and ranking metrics
- [x] **Performance Achievement**: 50.7% text‚Üíimage and 70.0% image‚Üítext R@1
- [x] **Embedding Storage**: Efficient caching for downstream tasks

### ‚úÖ Phase 3: Scalability Optimization (COMPLETE)
- [x] **FAISS Integration**: HNSW index for approximate nearest neighbor search
- [x] **Parameter Tuning**: Systematic optimization of accuracy vs speed trade-offs
- [x] **Performance Validation**: Sub-millisecond queries with zero quality loss
- [x] **Production Readiness**: Memory-efficient indexes ready for deployment

### ‚úÖ Phase 4: Advanced Training Exploration (COMPLETE)
- [x] **Fine-tuning Attempts**: Multiple approaches including conservative training
- [x] **Challenge Documentation**: Comprehensive analysis of catastrophic forgetting
- [x] **Production Decision**: Evidence-based choice of baseline model
- [x] **Lessons Learned**: Valuable insights into vision-language model training

### ‚úÖ Phase 5: Production Deployment (COMPLETE)
- [x] **FastAPI Service**: Professional REST API with 8 endpoints
- [x] **Request Validation**: Pydantic models with comprehensive error handling
- [x] **Performance Monitoring**: Built-in statistics and health checks
- [x] **Container Deployment**: Docker and docker-compose for production

## üöÄ Quick Start

### üê≥ Production Deployment
```bash
# Clone repository
git clone https://github.com/anshthamke87/clip-image-text-retrieval-for-captioned-media.git
cd clip-image-text-retrieval-for-captioned-media

# Deploy with Docker
cd service
docker-compose up --build

# Access API
curl http://localhost:8000/health
```

### üìö API Documentation
Once deployed, access:
- **Interactive API Docs**: http://localhost:8000/docs
- **Alternative Docs**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health

### üîç Usage Examples
```python
import requests

# Text-to-image search
response = requests.post(
    "http://localhost:8000/search/images",
    json={"text": "a dog playing in water", "k": 5}
)

# Image-to-text search
with open("image.jpg", "rb") as f:
    response = requests.post(
        "http://localhost:8000/search/text",
        files={"file": f},
        params={"k": 5}
    )

# Text encoding
response = requests.post(
    "http://localhost:8000/encode/text",
    json={"text": "example caption"}
)
```

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ data/                          # Dataset and processing
‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl               # Training data (28,315 entries)
‚îÇ   ‚îú‚îÄ‚îÄ val.jsonl                 # Validation data (6,065 entries)
‚îÇ   ‚îú‚îÄ‚îÄ test.jsonl                # Test data (6,075 entries)
‚îÇ   ‚îú‚îÄ‚îÄ images/                   # Processed images (8,091 files)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                 # Dataset documentation
‚îú‚îÄ‚îÄ artifacts/                    # Models and computed assets
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/              # Pre-computed embeddings
‚îÇ   ‚îú‚îÄ‚îÄ indexes/                 # FAISS search indexes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_hnsw_index.faiss
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_hnsw_index.faiss
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index_metadata.json
‚îÇ   ‚îú‚îÄ‚îÄ models/                  # Model checkpoints
‚îÇ   ‚îî‚îÄ‚îÄ project_state.json       # Project tracking
‚îú‚îÄ‚îÄ results/                     # Evaluation and analysis
‚îÇ   ‚îú‚îÄ‚îÄ zero_shot_baseline_results.json
‚îÇ   ‚îú‚îÄ‚îÄ faiss_parameter_tuning.json
‚îÇ   ‚îú‚îÄ‚îÄ final_model_results.json
‚îÇ   ‚îî‚îÄ‚îÄ production_api_results.json
‚îú‚îÄ‚îÄ reports/                     # Visualizations and analysis
‚îÇ   ‚îú‚îÄ‚îÄ faiss_tradeoff_analysis.png
‚îÇ   ‚îî‚îÄ‚îÄ baseline_vs_finetuning_analysis.png
‚îú‚îÄ‚îÄ service/                     # Production deployment
‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt         # Dependencies
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile              # Container definition
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml      # Orchestration
‚îÇ   ‚îú‚îÄ‚îÄ config.json             # Service configuration
‚îÇ   ‚îî‚îÄ‚îÄ README.md               # Deployment guide
‚îî‚îÄ‚îÄ README.md                    # This file
```

## üîß Technical Implementation

### Model Architecture
- **Vision Encoder**: OpenCLIP ViT-B/32 (86M parameters)
- **Text Encoder**: Transformer with 63M parameters  
- **Embedding Dimension**: 512-dimensional L2-normalized vectors
- **Training**: Pre-trained on 400M image-text pairs

### Search Infrastructure
- **Index Type**: Hierarchical Navigable Small World (HNSW)
- **Parameters**: M=16, efConstruction=200, optimized efSearch
- **Memory Usage**: <50MB for complete search indexes
- **Throughput**: >1000 queries per second per core

### API Features
- **Endpoints**: 8 RESTful endpoints with full OpenAPI documentation
- **Validation**: Comprehensive request/response validation with Pydantic
- **Caching**: Intelligent query result caching for performance
- **Monitoring**: Built-in health checks and usage statistics
- **Error Handling**: Professional error responses with detailed logging

## üìà Performance Analysis

### Benchmark Results
Our system achieves competitive performance with academic state-of-the-art while maintaining production requirements:

- **Accuracy**: 50.7% text‚Üíimage R@1 (competitive with published papers)
- **Speed**: Sub-millisecond search (production-grade performance)
- **Scalability**: Linear scaling with dataset size
- **Reliability**: Comprehensive error handling and monitoring

### Production Characteristics
- **Startup Time**: <30 seconds including model loading
- **Memory Usage**: <2GB RAM for complete system
- **CPU Efficiency**: Optimized for CPU-only deployment
- **Concurrency**: Async FastAPI handles multiple concurrent requests

## üéì Key Learning Outcomes

This project demonstrates mastery of:

### **Technical Skills**
- **Multimodal AI**: Vision-language model implementation and optimization
- **Information Retrieval**: Building efficient similarity search systems
- **ML Engineering**: End-to-end pipeline from data to deployment
- **API Development**: Production-grade REST service design
- **Containerization**: Docker-based deployment strategies

### **Engineering Practices**
- **Data Engineering**: Large-scale dataset processing and validation
- **Performance Optimization**: Systematic accuracy vs speed trade-offs
- **Production Deployment**: Complete CI/CD pipeline considerations
- **Documentation**: Comprehensive technical and user documentation
- **Testing**: Systematic validation and error handling

### **Research Insights**
- **Baseline Strength**: Pre-trained models often outperform fine-tuning
- **Fine-tuning Challenges**: Catastrophic forgetting in vision-language models
- **Production Trade-offs**: Balancing accuracy, speed, and reliability
- **System Design**: Building scalable ML systems from research prototypes

## üî¨ Research Contributions

### Novel Findings
- **Baseline Efficacy**: Demonstrated that OpenCLIP baseline (50.7% R@1) is competitive and stable
- **Fine-tuning Sensitivity**: Documented challenges with CLIP fine-tuning on domain-specific data
- **Production Optimization**: Systematic analysis of FAISS parameter optimization for CLIP embeddings

### Methodological Contributions
- **Evaluation Pipeline**: Comprehensive bi-directional retrieval evaluation
- **Deployment Framework**: Complete pipeline from research to production
- **Performance Analysis**: Detailed latency vs accuracy characterization

## üìä Future Enhancements

### Technical Improvements
- **Model Scaling**: Experiment with larger CLIP variants (ViT-L, ViT-H)
- **Multilingual Support**: Extend to non-English captions and queries
- **Domain Adaptation**: Explore safe fine-tuning approaches for specific domains
- **Real-time Learning**: Implement online learning for user feedback

### System Enhancements
- **Auto-scaling**: Kubernetes deployment with horizontal scaling
- **Advanced Caching**: Redis-based distributed caching
- **Monitoring**: Comprehensive logging and metrics with Prometheus
- **Security**: Authentication, rate limiting, and input sanitization

## üìß Contact

**Ansh Thamke**
- Email: at3841@columbia.edu
- GitHub: [@anshthamke87](https://github.com/anshthamke87)
- LinkedIn: [Connect for collaboration opportunities]

## üìÑ Citation

If you use this work in your research, please cite:

```bibtex
@software{thamke2024clip_retrieval,
  title={CLIP Image-Text Retrieval Engine for Captioned Media},
  author={Thamke, Ansh},
  year={2024},
  url={https://github.com/anshthamke87/clip-image-text-retrieval-for-captioned-media},
  note={Complete end-to-end implementation with production deployment}
}
```

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**üéâ Built with ‚ù§Ô∏è for advancing multimodal AI systems and production ML deployment**

*This project represents a complete journey from research idea to production deployment, demonstrating both technical depth and engineering excellence.*
